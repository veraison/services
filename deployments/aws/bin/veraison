#!/usr/bin/env python
# pyright: reportOptionalMemberAccess=false
# pyright: reportOptionalSubscript=false
# pyright: reportOperatorIssue=false
import argparse
import asyncio
import getpass
import inspect
import json
import logging
import os
import pprint
import random
import re
import shlex
import shutil
import socket
import stat
import string
import sys
import time
from asyncio.subprocess import Process, PIPE
from collections import defaultdict
from copy import copy
from datetime import UTC, datetime, timedelta
from urllib.parse import urlparse

import boto3
import dateutil
import fabric
import pytimeparse
import requests
import xdg.BaseDirectory
import yaml
from botocore.exceptions import ClientError
from envsubst import envsubst
from jwcrypto import jwk
from oauthlib.oauth2 import LegacyApplicationClient
from requests_oauthlib import OAuth2Session
from sqlitedict import SqliteDict


COLOR_DARK_GREY = '\x1b[38;5;245m'
COLOR_MEDIUM_GREY = '\x1b[38;5;250m'
COLOR_GREY = '\x1b[38;20m'
COLOR_GREEN = '\x1b[38;5;2m'
COLOR_PURPLE = '\x1b[38;5;96m'
COLOR_YELLOW = '\x1b[33;20m'
COLOR_RED = '\x1b[31;20m'
COLOR_BOLD_RED = '\x1b[31;1m'
COLOR_RESET = '\x1b[0m'
']]]]]]]]'  # "close" the brackets above to fix nvim's auto-indent

LEVEL_REGEX = re.compile(r'^(?P<level>DEBUG|INFO|WARN|WARNING|ERROR|FATAL)\t(?P<rest>.*)')
LOG_COLORS = {
    'DEBUG': COLOR_DARK_GREY,
    'INFO': COLOR_MEDIUM_GREY,
    'WARN': COLOR_YELLOW,
    'WARNING': COLOR_YELLOW,
    'ERROR': COLOR_RED,
    'FATAL': COLOR_BOLD_RED,
}


class Aws:

    client_name_map = {
        'cf': 'cloudformation',
    }

    def __init__(self, **kwargs):
        self.session = boto3.Session(**kwargs)
        self.clients = {}

    def close(self):
        for client in self.clients.values():
            client.close()
        self.clients = {}

    def __getattr__(self, name):
        client_name = self.client_name_map.get(name, name)
        if client_name not in self.clients:
            self.clients[client_name] = self.session.client(client_name)
        return self.clients[client_name]

    def __del__(self):
        self.close()


def randomword(n=32, use_punctuation=False):
    alphabet = string.ascii_letters + string.digits
    if use_punctuation:
        alphabet += string.punctuation
    return ''.join(random.choice(alphabet) for _ in range(n))


def camel_to_kebab(name: str):
    words = []
    current = ''
    for char in name:
        if char.isupper():
            if current:
                words.append(current)
            current = char.lower()
        else:
            current += char

    if current:
        words.append(current)

    return '-'.join(words)


def get_public_ip_address():
    resp = requests.get('http://ifconfig.me')
    if resp.status_code != 200:
        raise RuntimeError(
                f'could not access http://ifconfig.me: {resp.reason} ({resp.status_code})')
    return resp.text


def authorize_ports_for_address(aws, group_id, addr, ports, tag, deployment_tags):
    permissions = []
    for port in ports:
        permissions.append({
            'FromPort': port,
            'ToPort': port,
            'IpProtocol': 'tcp',
            'IpRanges': [{'CidrIp': f'{addr}/32'}],
        })

    tags = copy(deployment_tags)
    tags.append({'Key': 'dynamic-address', 'Value': tag})

    aws.ec2.authorize_security_group_ingress(
        GroupId=group_id,
        IpPermissions=permissions,
        TagSpecifications=[
            {
                'ResourceType': 'security-group-rule',
                'Tags': tags,
            }
        ],
    )


def revoke_security_group_rules_by_tag(aws, group_id, tag):
    resp = aws.ec2.describe_security_group_rules(
        Filters=[
            {
                'Name': 'group-id',
                'Values': [group_id],
            },
            {
                'Name': 'tag:dynamic-address',
                'Values': [tag],
            }
        ],
    )

    rule_ids = [sgr['SecurityGroupRuleId'] for sgr in resp['SecurityGroupRules']]
    if not rule_ids:
        return

    aws.ec2.revoke_security_group_ingress(
        GroupId=group_id,
        SecurityGroupRuleIds=rule_ids,
    )


def colorize_log_line(line):
    match = LEVEL_REGEX.search(line)
    if match is None:
        raise ValueError(f'Could find log level in line: {line}')

    level = match.group('level')
    rest = match.group('rest')

    color = LOG_COLORS[level]
    return f'{color}{level}{COLOR_RESET}\t{rest}'


def command_create_packer_security_group(cmd, deployment_name):
    vpc_id = command_get(cmd, 'vpc.vpc-id', 'VPC stack been created')

    cmd.logger.debug('creating Packer security group...')
    resp = cmd.aws.ec2.create_security_group(
        GroupName='veraison-packer',
        Description='Temporary security group for creating Veraison packer imeages',
        VpcId=vpc_id,
        TagSpecifications=[
            {
                'ResourceType': 'security-group',
                'Tags': [
                    {'Key': 'veraison-deployment', 'Value': deployment_name},
                    {'Key': 'Class', 'Value': 'packer'},
                ],
            }
        ],
    )

    cmd.logger.debug('obtaining public IP address for localhost...')
    my_addr = get_public_ip_address()

    cmd.logger.debug('Adding access rule for SSH from localhost...')
    authorize_ports_for_address(
        cmd.aws, resp['GroupId'], my_addr, [22], cmd.cache.user_tag,
        deployment_tags=[
            {'Key': 'veraison-deployment', 'Value': deployment_name},
            {'Key': 'Class', 'Value': 'packer'},
        ],
    )


def command_delete_packer_security_group(cmd, deployment_name):
    cmd.logger.debug(f'looking for packer group(s) for deployment {deployment_name}...')
    resp = cmd.aws.ec2.describe_security_groups(
        Filters=[
            {'Name': 'tag:veraison-deployment', 'Values': [deployment_name]},
            {'Name': 'tag:Class', 'Values': ['packer']},
        ]
    )

    group_ids = [sgr['GroupId'] for sgr in resp['SecurityGroups']]
    if not group_ids:
        cmd.logger.debug(f'no packer group found for deployment {deployment_name}')
        return

    for group_id in group_ids:
        cmd.logger.debug(f'deleting security group {group_id}...')
        cmd.aws.ec2.delete_security_group(GroupId=group_id)


def command_update_dynamic_address_rules(cmd, deployment_name, group_names, tag, ports):
    resp = aws.ec2.describe_security_groups(
        Filters=[
            {
                'Name': 'tag:veraison-deployment',
                'Values': [deployment_name],
            },
            {
                'Name': 'group-name',
                'Values': group_names,
            },
        ]
    )

    group_ids = [sgr['GroupId'] for sgr in resp['SecurityGroups']]
    if not group_ids:
        cmd.logger.info(f'no groups found for deployment {deployment_name}')
        return

    my_addr = get_public_ip_address()
    for group_id in group_ids:
        revoke_security_group_rules_by_tag(cmd.aws, group_id, tag)
        authorize_ports_for_address(
                cmd.aws, group_id, my_addr, ports, tag,
                deployment_tags=[{'Key': 'veraison-deployment', 'Value': deployment_name}])


def command_get_hosted_zone(cmd, dns_name):
    cmd.logger.debug(f'looking up hosted zone of {dns_name}...')
    resp = cmd.aws.route53.list_hosted_zones()
    for hz in resp['HostedZones']:
        if hz['Name'] == f'{dns_name}.':
            return hz['Id'].strip('/hostedzone/')
    else:
        cmd.fail(f'Could not find hosted zone for {dns_name}')


def get_stack_instances_info(aws, stack_name):
    resp = aws.cf.describe_stack_resources(StackName=stack_name)

    info = {}

    for resource in resp['StackResources']:
        if resource['ResourceType'] == 'AWS::EC2::Instance':
            instance_id = resource['PhysicalResourceId']
            update_info_with_ec2_instance(aws, info, instance_id)
        elif resource['ResourceType'] == 'AWS::RDS::DBInstance':
            instance_id = resource['PhysicalResourceId']
            update_info_with_rds_instance(aws, info, instance_id)

    return info


def update_info_with_rds_instance(aws, info, instance_id):
    resp = aws.rds.describe_db_instances(DBInstanceIdentifier=instance_id)
    instance = resp['DBInstances'][0]

    instance_name = None
    for tag in instance['TagList']:
        if tag['Key'] == 'deployment-instance-name':
            instance_name = tag['Value']
            break

    if not instance_name:
        # if not explicitly named, assume it's the sole RDS instance for the deployment
        instance_name = 'rds'

    info[instance_name] = {
        'id': instance_id,
        'dns_name': instance['Endpoint']['Address'],
        'port': instance['Endpoint']['Port'],
    }


def update_info_with_ec2_instance(aws, info, instance_id):
    resp = aws.ec2.describe_instances(InstanceIds=[instance_id])
    instance = resp['Reservations'][0]['Instances'][0]

    instance_name = None
    fallback_name = instance_id
    for tag in instance['Tags']:
        if tag['Key'] == 'deployment-instance-name':
            instance_name = tag['Value']
            break
        elif tag['Key'] == 'Name':
            fallback_name = tag['Value']

    if not instance_name:
        instance_name = fallback_name

    pub_iface = instance['NetworkInterfaces'][0]['Association']

    info[instance_name] = {
            'id': instance_id,
            'dns_name': pub_iface["PublicDnsName"],
            'ip_address': pub_iface["PublicIp"],
    }


def get_ami_id(aws, name):
    resp = aws.ec2.describe_images(Owners=['self'])
    for image in resp['Images']:
        if image['Name'] == name:
            return image['ImageId']


def run_in_shell(cmd, should_log):
    logger = logging.getLogger('shell')
    if should_log:
        logger.setLevel(logging.DEBUG)

    loop = asyncio.new_event_loop()
    try:
        return loop.run_until_complete(_run_in_shell_teed(cmd, logger))
    finally:
        loop.close()


async def _run_in_shell_teed(cmd, logger):
    process: Process = await asyncio.create_subprocess_shell(
            cmd, stdout=PIPE, stderr=PIPE, cwd=os.getcwd())

    stdout_buf, stderr_buf = [], []
    tasks = {
        asyncio.Task(process.stdout.readline()): (process.stdout, stdout_buf),
        asyncio.Task(process.stderr.readline()): (process.stderr, stderr_buf),
    }

    while tasks:
        done, _ = await asyncio.wait(
                tasks, return_when=asyncio.FIRST_COMPLETED)  # pyright: ignore[reportCallIssue]
        for future in done:
            stream, buf = tasks.pop(future)
            line = future.result()
            if line:
                line = line.decode()
                buf.append(line)
                logger.debug(line.rstrip('\n'))
                tasks[asyncio.Task(stream.readline())] = stream, buf  # pyright: ignore[reportOptionalMemberAccess]

    rc = await process.wait()
    return rc, ''.join(stdout_buf), ''.join(stderr_buf)


def command_get_config(cmd, name):
    try:
        return cmd.cache[name]
    except KeyError:
        cmd.fail(f'could not get {name} from cache; has configure command been run?')


def command_get(cmd, path, precondition=None):
    val = cmd.cache.get(path)
    if val is None:
        if precondition:
            cmd.fail(f'could not get {path} from cache; has {precondition}?')
        else:
            cmd.fail(f'could not get {path} from cache')
    return val


def command_instantiate_template(cmd, deployment_name, src_path, out_dir='/tmp'):
    with open(src_path) as fh:
        os.environ['DEPLOYMENT_NAME'] = deployment_name
        instantiated_template_body = envsubst(fh.read())
        instantiated_template_body = instantiated_template_body.replace('$@{', '${')

    basename = os.path.basename(src_path).removesuffix('.template')
    out_path = os.path.join(out_dir, f'{deployment_name}-{basename}')
    cmd.logger.debug(f'writing {out_path}')
    with open(out_path, 'w') as wfh:
        wfh.write(instantiated_template_body)

    return out_path


def command_create_image(cmd, args, name, packer_params=None):
    if not shutil.which('packer'):
        cmd.fail('packer must be installed on the system')

    if not os.path.isfile(args.template):
        cmd.fail(f'template {args.template} does not exist')

    full_name = f'{args.deployment_name}-{name}'
    cmd.logger.info(f'creating image: {full_name}...')

    command_create_packer_security_group(cmd, args.deployment_name)

    try:
        cmd.logger.debug('checking for existing AMI with that name...')
        existing_id = get_ami_id(cmd.aws, full_name)
        if existing_id:
            if not args.force:
                cmd.fail(f'image {full_name} already exits (use -f to overwrite)')
            cmd.logger.info('removing existing image...')
            cmd.aws.ec2.deregister_image(ImageId=existing_id)

        cmd.logger.info('building using packer...')
        region = command_get_config(cmd, 'region')
        vpc_id = command_get(cmd, 'vpc.vpc-id', 'VPC stack been created')
        subnet_id = command_get(cmd, 'vpc.public-subnet-a-id', 'VPC stack been created')

        params_dict = {
            'ami_name': full_name,
            'deployment_name': args.deployment_name,
            'instance_type': args.instance_type,
            'region': region,
            'vpc_id': vpc_id,
            'subnet_id': subnet_id,
        }
        params_dict.update(packer_params or {})

        params_str = ' '.join(f'-var {k}={v}' for k, v in params_dict.items() if v is not None)

        packer_cmd = f'packer build {params_str} {args.template}'
        cmd.logger.debug(packer_cmd)
        exit_code, stdout, stderr = run_in_shell(packer_cmd, args.verbose)
        if exit_code:
            cmd.fail_shell('packer', exit_code, stdout, stderr)

        regex = re.compile(r'AMI: (?P<id>ami-\w+)')
        match = regex.search(stdout)
        if not match:
            cmd.fail('could not find AMI ID in packer output')

        images = cmd.cache.get('images', {})
        images[name] = match.group('id')
        cmd.cache['images'] = images
    finally:
        command_delete_packer_security_group(cmd, args.deployment_name)

    cmd.logger.info('done.')


def command_create_stack(
        cmd, deployment_name, stack_name, template_path, extra_params,
        wait_period=60, capabilities=[]):
    cmd.logger.info(f'creating stack {stack_name}...')
    stack_full_name = f'{deployment_name}-{stack_name}'

    # doing this to be compatible with AWS CLI which specifies the template path as
    # file://path/to/template.
    url = urlparse(template_path)
    cmd.logger.debug(f'template: {url.path}')
    with open(url.path) as fh:
        template = fh.read()

    params = [
        {'ParameterKey': 'DeploymentName', 'ParameterValue': deployment_name},
    ]
    params.extend(extra_params)

    cmd.logger.debug(f'using params {params}')
    resp = cmd.aws.cf.create_stack(
        StackName=stack_full_name,
        TemplateBody=template,
        Parameters=params,
        OnFailure='DELETE',
        Capabilities=capabilities,
    )
    cmd.logger.debug(f'stack ID: {resp["StackId"]}')

    cmd.logger.info('waiting for the stack creation to complete...')
    resp = cmd.aws.cf.describe_stacks(StackName=stack_full_name)
    while resp['Stacks'][0]['StackStatus'] == 'CREATE_IN_PROGRESS':
        time.sleep(wait_period)
        resp = cmd.aws.cf.describe_stacks(StackName=stack_full_name)

    stack_status = resp['Stacks'][0]['StackStatus']
    if stack_status == 'CREATE_COMPLETE':
        outputs = {}
        for o in resp['Stacks'][0].get('Outputs', []):
            outputs[camel_to_kebab(o['OutputKey'])] = o['OutputValue']
        cmd.cache[stack_name] = outputs

        if outputs:
            cmd.logger.debug('outputs:')
            for k, v in outputs.items():
                cmd.logger.debug(f'    {k}: {v}')

        cmd.logger.info('done.')
    else: # stack_status != 'CREATE_COMPLETE'
        cmd.logger.error(f'creation failed: {stack_status}')
        resp = cmd.aws.cf.describe_stack_events(StackName=stack_full_name)

        for event in resp['StackEvents']:
            if event['ResourceStatus'] != 'CREATE_FAILED':
                continue
            status = event['ResourceStatus']
            reason = event.get("ResourceStatusReason", '')
            cmd.logger.error(f'{status} {reason}')

        cmd.logger.info('waiting for the rollback to complete...')
        resp = cmd.aws.cf.describe_stacks(StackName=stack_full_name)
        while resp['Stacks'][0]['StackStatus'] == 'DELETE_IN_PROGRESS':
            time.sleep(wait_period)
            resp = cmd.aws.cf.describe_stacks(StackName=stack_full_name)

        cmd.fail(f'could not create stack {stack_name}')


def command_get_ec2_instances(cmd):
    resp = cmd.aws.ec2.describe_instances(
        Filters=[
            {
                'Name': 'tag:veraison-deployment',
                'Values': [
                    args.deployment_name,
                ],
            },
        ],
        MaxResults=64,
    )

    instances = defaultdict(list)
    for reservation in resp['Reservations']:
        for instance in reservation['Instances']:
            for tag in instance['Tags']:
                if (tag['Key'] == 'deployment-instance-name' and
                        instance['PrivateDnsName']):
                    instances[tag['Value']].append(instance)
                    break

    return instances


def command_connect_sentinel(cmd, user='ubuntu'):
    host = command_get(cmd, 'support.sentinel-dns-name', 'RDS stack been created')
    key_path = command_get(cmd, 'key.path', 'a key pair been created')
    return fabric.Connection(
        host,
        user=user,
        connect_kwargs={
            'key_filename': key_path,
        },
    )


def command_connect_ec2_instance(cmd, host, user='ubuntu'):
    key_path = command_get(cmd, 'key.path', 'a key pair been created')
    proxy = command_connect_sentinel(cmd, user=user)
    return fabric.Connection(
        host,
        user=user,
        connect_kwargs={
            'key_filename': key_path,
        },
        gateway=proxy,
    )


def command_sentinel(cmd, what, verbose=False, **kwargs):
    flags = ''
    if verbose:
        flags += ' --verbose'
    with command_connect_sentinel(cmd) as con:
        return con.run(f'veraison{flags} {what}', **kwargs)


def command_restart_cw_agents(cmd, delete_logs=False):
    cmd.logger.debug('looking up service instances...')
    instances = command_get_ec2_instances(cmd)
    for service in ['provisioning', 'verification', 'management']:
        for instance in instances[service]:
            host = instance['PrivateDnsName']
            cwa_path = '/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl'

            cmd.logger.debug(f'connecting to {host}...')
            con = command_connect_ec2_instance(cmd, host)

            if delete_logs:
                cmd.logger.debug(f'clearing veraison logs...')
                for service in ['vts', 'provisioning', 'verification', 'management']:
                    con.run(f'sudo -u veraison /usr/bin/bash -c \'echo "" > '
                            f'/opt/veraison/logs/{service}-stdout.log\'',
                        echo=False, hide=True)

            cmd.logger.debug(f'restarting agent on {host}...')
            con.run(f'sudo {cwa_path} -a stop', echo=False, hide=True)
            con.run(f'sudo {cwa_path} -a start', echo=False, hide=True)


def command_get_cw_logs(cmd, deployment_name, service, start=None, end=None):
    kwargs = {}

    start_time = convert_timestamp(start)
    if start_time:
        kwargs['startTime'] =  int(start_time * 1000)

    end_time = convert_timestamp(end)
    if end_time:
        kwargs['endTime'] = int(end_time * 1000)

    def iter_events():
        try:
            resp = cmd.aws.logs.filter_log_events(
                    logGroupName=f'{deployment_name}-{service}',
                    **kwargs,
            )
            while resp['events']:
                for event in resp['events']:
                    yield event

                token = resp.get('nextToken')
                if token:
                    resp = cmd.aws.logs.filter_log_events(
                            logGroupName=f'{deployment_name}-{service}',
                            nextToken=token,
                            **kwargs,
                    )
        except cmd.aws.logs.exceptions.ResourceNotFoundException:
            pass

    return iter_events()


def command_get_elasticache_servers(cmd):
    cmd.logger.debug('obtaining ElastiCache servers...')
    support = command_get(cmd, 'support', 'support stack been deployed')
    cluster_id = support['elasti-cache-cluster-id']

    resp = cmd.aws.elasticache.describe_cache_clusters(
        CacheClusterId=cluster_id,
        ShowCacheNodeInfo=True,
    )

    servers = []
    for node in resp['CacheClusters'][0]['CacheNodes']:
        ep = node['Endpoint']
        servers.append(f'{ep['Address']}:{ep['Port']}')

    return servers


def convert_timestamp(value):
    if value is None:
        return None
    elif isinstance(value, datetime):
        return value.timestamp()
    elif isinstance(value, timedelta):
        return (datetime.now() - value).timestamp()
    elif isinstance(value, int):
        return value
    elif isinstance(value, str):
       value = value.lower()
       if value.endswith(' ago'):
           value = value[:-4]

       if value == 'today':
           ret = datetime.now()
           ret = ret.replace(hour=0, minute=0, second=0)
           return ret.timestamp()
       elif value == 'yesterday':
           ret = datetime.now() - timedelta(days=1)
           ret = ret.replace(hour=0, minute=0, second=0)
           return ret.timestamp()
       else:
           # first, assume absolute date
           try:
               ret = dateutil.parser.parse(value) # pyright: ignore
               return ret.timestamp()
           except dateutil.parser.ParserError: # pyright: ignore
               pass

           #' if not absolute date, then time delta
           seconds = pytimeparse.parse(value)
           if not seconds:
               raise ValueError(f'invalid time period string "{value}"')

           ret = datetime.now() - timedelta(seconds=seconds)
           return ret.timestamp()
    else: # not any expected type
       raise ValueError(f'invalid time period value {value}')


def print_cw_event(event):
    ts = datetime.fromtimestamp(event['ingestionTime']/1000, UTC)
    inst = event['logStreamName']

    tspart = f'{COLOR_GREEN}{ts.strftime('%Y-%m-%d %H:%M:%S')}{COLOR_RESET}'
    instpart = f'{COLOR_PURPLE}{inst}{COLOR_RESET}'
    line = colorize_log_line(event['message'])
    print(f'{tspart} {instpart} {line}')


class DeploymentCache:

    @property
    def dir(self):
        return os.path.dirname(self.path)

    @property
    def certs_dir(self):
        return os.path.join(self.dir, 'certs')

    @property
    def ca_cert_path(self):
        return os.path.join(self.certs_dir, 'rootCA.crt')

    @property
    def ca_key_path(self):
        return os.path.join(self.certs_dir, 'rootCA.key')

    @property
    def user_tag(self):
        return self.db.get('user_tag', self.default_user_tag)

    def __init__(self, name, cache_dir=None):
        if not name:
            raise ValueError('name cannot be empty')
        self.name = name
        if cache_dir is None:
            cache_dir = xdg.BaseDirectory.save_data_path('veraison/aws')
        self.path = os.path.join(cache_dir, f'{self.name}.db')
        self.db = SqliteDict(self.path)
        self.default_user_tag = f'{socket.gethostname()}-{getpass.getuser()}'

    def get(self, key, default=None):
        parts = key.split('.')
        entry = self.db
        path = ''

        try:
            for part in parts[:len(parts)-1]:
                entry = self._access_member(entry, part, path)
                path = path + '.' + part
            return self._access_member(entry, parts.pop(), path)
        except (KeyError, IndexError):
            return default

    def as_dict(self):
        return {k: v for k, v in self.db.items()}

    def close(self):
        self.db.close()

    def _access_member(self, entry, part, path):
        try: # if part is an int, assume list entry
            idx = int(part)
            if (len(entry)-1) > idx:  # pyright: ignore[reportArgumentType]
                if path:
                    raise IndexError(f'{path}.{idx}')
                else:
                    raise IndexError(f'{idx}')
            return entry[idx]
        except ValueError:  # part not an int, assume dict entry
            try:
                return entry[part]
            except KeyError as e:
                if path:
                    raise KeyError(f'{path}.{part}')
                else:
                    raise e

    def __contains__(self, key):
        return key in self.db

    def __setitem__(self, key, value):
        self.db[key] = value
        self.db.commit()

    def __getitem__(self, key):
        return self.db[key]

    def __delitem__(self, key):
        try:
            del self.db[key]
            self.db.commit()
        except KeyError:
            pass


# {{{ commands

class BaseCommand:

    name = None
    desc = None
    aliases = []

    def __init__(self, aws, fail_with_error=True):
        self.aws = aws
        self.logger = logging.getLogger(self.name)
        self.fail_with_error = fail_with_error
        self.cache = None

    def register(self, subparsers):
        parser = subparsers.add_parser(self.name, help=self.desc, aliases=self.aliases)
        self.update_arguments(parser)

    def execute(self, args):
        if args.verbose and args.quiet:
            self.fail('only one of -v/--verbose or -q/--quiet may be specfied at a time')
        if args.verbose:
            self.logger.setLevel(logging.DEBUG)
        elif args.quiet:
            self.logger.setLevel(logging.WARNING)

        self.cache = DeploymentCache(args.deployment_name, args.cache_dir)
        try:
            self.run(args)
        finally:
            self.cache.close()

    def fail(self, message):
        write = self.logger.error if self.fail_with_error else self.logger.info
        write(message)
        raise RuntimeError(f'command {self.name} failed.')

    def fail_shell(self, command, exit_code, stdout, stderr):
        stdout_file = f'/tmp/{args.deployment_name}-{command}-failure.stdout'
        with open(stdout_file, 'w') as wfh:
            wfh.write(stdout)

        stderr_file = f'/tmp/{args.deployment_name}-{command}-failure.stderr'
        with open(stderr_file, 'w') as wfh:
            wfh.write(stderr)

        self.fail(f'{command} failed with {exit_code}'
                  f'\n\tSTDOUT is in {stdout_file}\n\tSTDERR is in {stderr_file}')

    def update_arguments(self, parser):
        pass

    def run(self, *args, **kwargs):
        raise NotImplementedError()


class CreateSupportStackCommand(BaseCommand):

    name = 'create-support-stack'
    desc = 'create deployment\'s cloudformation stack running RDS instance'

    def run(self, args):
        template_path = os.path.abspath(os.path.join(
            os.path.dirname(__file__),
            '../templates/stack-support.yaml',
        ))

        vpc_cidr = command_get_config(self, 'vpc-cidr')
        admin_cidr = command_get_config(self, 'admin-cidr')
        region = command_get_config(self, 'region')
        key_name = command_get(self, 'key.name', 'a key pair been created')
        vpc_id = command_get(self, 'vpc.vpc-id', 'VPC stack been created')
        dns_name = command_get_config(self, 'dns-name')
        hz_id = command_get_hosted_zone(self, dns_name)
        sentinel_image = command_get(self, 'images.sentinel', 'sentinel image been created')
        public_subnet = command_get(self, 'vpc.public-subnet-a-id', 'VPC stack been created')
        rds_subnets = [
            self.cache.get('vpc.private-subnet-a-id'),
            self.cache.get('vpc.private-subnet-b-id'),
        ]

        self.logger.debug('checking linked ElastiCache role...')
        resp = self.aws.iam.list_roles(
            PathPrefix='/aws-service-role/elasticache.amazonaws.com/',
        )
        if not resp['Roles']:
            self.logger.debug('creating linked role for ElastiCache...')
            self.aws.iam.create_service_linked_role(
                AWSServiceName='elasticache.amazonaws.com',
                Description='Service-linked role for ElastiCache to access Veraison deployments',
            )

        self.logger.debug('generating admin password for Postgres and writing into cache...')
        # note: not using punctuation in the initial password, as it will be passed through
        # multiple shells, environments, and tools, and we don't want to
        # warry about correctrly escaping everything at every stage. Using a longer string
        # to compensate.
        password = randomword(40)
        self.cache['postgres_admin_password'] = password

        extra_params = [
            {'ParameterKey': 'VpcId', 'ParameterValue': vpc_id},
            {'ParameterKey': 'KeyName', 'ParameterValue': key_name},
            {'ParameterKey': 'SubnetCidr', 'ParameterValue': vpc_cidr},
            {'ParameterKey': 'RdsSubnets', 'ParameterValue': ','.join(rds_subnets)},  # pyright: ignore
            {'ParameterKey': 'RdsPassword', 'ParameterValue': password},  # pyright: ignore
            {'ParameterKey': 'KeyName', 'ParameterValue': key_name},
            {'ParameterKey': 'PublicSubnet', 'ParameterValue': public_subnet},
            {'ParameterKey': 'ParentDomain', 'ParameterValue': dns_name},
            {'ParameterKey': 'HostedZoneId', 'ParameterValue': hz_id},
            {'ParameterKey': 'SentinelImage', 'ParameterValue': sentinel_image},
            {'ParameterKey': 'AdminCidr', 'ParameterValue': admin_cidr},
            {'ParameterKey': 'Region', 'ParameterValue': region},
        ]

        command_create_stack(cmd, args.deployment_name, 'support',
                             template_path, extra_params, args.wait_period)


class CreateServicesStackCommand(BaseCommand):

    name = 'create-services-stack'
    desc = '''create deployment\'s CloudFormation stack running Veraison services
           on auto-scaled instances'''

    def run(self, args):
        template_path = os.path.abspath(os.path.join(
            os.path.dirname(__file__),
            '../templates/stack-services.yaml',
        ))

        region = command_get_config(self, 'region')
        dns_name = command_get_config(self, 'dns-name')
        ports = command_get_config(self, 'ports')
        scaling = command_get_config(self, 'scaling')
        hz_id = command_get_hosted_zone(self, dns_name)
        admin_cidr = command_get_config(self, 'admin-cidr')
        iam = command_get_config(self, 'iam')
        key_name = command_get(self, 'key.name', 'a key pair been created')
        services_image = command_get(self, 'images.services', 'services image been created')
        keycloak_image = command_get(self, 'images.keycloak', 'keycloak image been created')
        vpc_id = command_get(self, 'vpc.vpc-id', 'VPC stack been created')
        pub_subnet_a_id = command_get(self, 'vpc.public-subnet-a-id', 'VPC stack been created')
        pub_subnet_b_id = command_get(self, 'vpc.public-subnet-b-id', 'VPC stack been created')
        pub_subnet_a_cidr = command_get(self, 'vpc.public-subnet-a-cidr',
                                        'VPC stack been created')
        pub_subnet_b_cidr = command_get(self, 'vpc.public-subnet-b-cidr',
                                        'VPC stack been created')
        priv_subnet_a_id = command_get(self, 'vpc.private-subnet-a-id', 'VPC stack been created')
        priv_subnet_b_id = command_get(self, 'vpc.private-subnet-b-id', 'VPC stack been created')
        sig_key_arn = command_get(self, 'signer-key-arn', 'signer key been created')

        self.logger.debug(f'looking up certificate for {dns_name}...')
        cert_arn = None
        resp = self.aws.acm.list_certificates(CertificateStatuses=['ISSUED'])
        for cert_summary in resp['CertificateSummaryList']:
            if cert_summary['DomainName'] == dns_name:
                cert_arn = cert_summary['CertificateArn']
                break
        else:
            self.fail(f'Could not find certificate for {dns_name}')

        extra_params = [
            {'ParameterKey': 'VpcId', 'ParameterValue': vpc_id},
            {'ParameterKey': 'AdminCidr', 'ParameterValue': admin_cidr},
            {'ParameterKey': 'KeyName', 'ParameterValue': key_name},
            {'ParameterKey': 'CombinedImage', 'ParameterValue': services_image},
            {'ParameterKey': 'PublicSubnetA', 'ParameterValue': pub_subnet_a_id},
            {'ParameterKey': 'PublicSubnetB', 'ParameterValue': pub_subnet_b_id},
            {'ParameterKey': 'PublicSubnetACidr', 'ParameterValue': pub_subnet_a_cidr},
            {'ParameterKey': 'PublicSubnetBCidr', 'ParameterValue': pub_subnet_b_cidr},
            {'ParameterKey': 'PrivateSubnetA', 'ParameterValue': priv_subnet_a_id},
            {'ParameterKey': 'PrivateSubnetB', 'ParameterValue': priv_subnet_b_id},

            {'ParameterKey': 'ServiceCidr', 'ParameterValue': admin_cidr},
            {'ParameterKey': 'Region', 'ParameterValue': region},

            {'ParameterKey': 'ParentDomain', 'ParameterValue': dns_name},
            {'ParameterKey': 'HostedZoneId', 'ParameterValue': hz_id},
            {'ParameterKey': 'CertificateArn', 'ParameterValue': cert_arn},
            {'ParameterKey': 'KeycloakImage', 'ParameterValue': keycloak_image},

            {'ParameterKey': 'ProvisioningPort', 'ParameterValue': str(ports['provisioning'])},
            {'ParameterKey': 'VerificationPort', 'ParameterValue': str(ports['verification'])},
            {'ParameterKey': 'ManagementPort', 'ParameterValue': str(ports['management'])},
            {'ParameterKey': 'KeycloakPort', 'ParameterValue': str(ports['keycloak'])},

            {'ParameterKey': 'ScalingMinSize', 'ParameterValue': str(scaling['min-size'])},
            {'ParameterKey': 'ScalingMaxSize', 'ParameterValue': str(scaling['max-size'])},
            {'ParameterKey': 'ScalingCpuUtilTarget',
             'ParameterValue': str(scaling['cpu-util-target'])},
            {'ParameterKey': 'ScalingRequestCountTarget',
             'ParameterValue': str(scaling['request-count-target'])},

            {'ParameterKey': 'RoleName', 'ParameterValue': iam['logger-role-name']},
            {'ParameterKey': 'InstanceProfileName',
             'ParameterValue': iam['instance-profile-name']},
            {'ParameterKey': 'PermissionBoundaryArn',
             'ParameterValue': iam['permission-boundary-arn']},
            {'ParameterKey': 'SignerKeyArn', 'ParameterValue': sig_key_arn},
        ]

        command_create_stack(cmd, args.deployment_name, 'services',
                             template_path, extra_params, args.wait_period,
                             capabilities=['CAPABILITY_NAMED_IAM'])


class DeleteStackCommand(BaseCommand):

    name = 'delete-stack'
    desc = 'delete a previously created stack'

    def update_arguments(self, parser):
        parser.add_argument('name')

    def run(self, args):
        stack_full_name = f'{args.deployment_name}-{args.name}'
        self.logger.info(f'deleting stack {stack_full_name}...')

        self.aws.cf.delete_stack(StackName=stack_full_name)
        try:
            self.logger.debug('waiting for the stack deletion to complete...')
            resp = self.aws.cf.describe_stacks(StackName=stack_full_name)
            while resp['Stacks'][0]['StackStatus'] == 'DELETE_IN_PROGRESS':
                time.sleep(args.wait_period)
                resp = self.aws.cf.describe_stacks(StackName=stack_full_name)
        except ClientError as e:
            if 'does not exist' not in str(e):
                raise e

        del self.cache[args.name]

        self.logger.info('done.')


class UpdateSecurityGroupsCommand(BaseCommand):

    name = 'update-security-groups'
    desc = 'update security group(s) in deployment with current host\'s IP address'

    def update_arguments(self, parser):
        parser.add_argument('-p', '--ports', action=StoreIntList,
                            default=[11111, 9443, 10443, 8443, 5432, 22])

    def run(self, args):
        self.logger.info('updating deployment security groups with IP address for this host...')

        names = [
            f'{args.deployment_name}-sentinel-intance-sg'
        ]

        try:
            command_update_dynamic_address_rules(
                    self, args.deployment_name, names, self.cache.user_tag, args.ports)
        except Exception as e:
            self.fail(e)

        self.logger.info('done.')


class CreateServicesImageCommand(BaseCommand):

    name = 'create-services-image'
    desc = 'create IMA image for the Veraison services EC2 instances'

    def update_arguments(self, parser):
        parser.add_argument('-D', '--deb')
        parser.add_argument(
                '-c', '--services-config-template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/services-config.yaml.template',
            )),
        )
        parser.add_argument(
                '-C', '--cloudwatch-config-template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/cw-services-logging-conf.json.template',
            )),
        )
        parser.add_argument(
            '-t', '--template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/image-services.pkr.hcl',
            )),
        )
        parser.add_argument('-T', '--instance-type')

    def run(self, args):
        deb_path = args.deb or self.cache['deb']
        if not os.path.isfile(deb_path):
            self.fail(f'{deb_path} does not exist')
        self.cache['deb'] = deb_path

        ports = command_get_config(self, 'ports')
        retention_days = command_get_config(self, 'cw-log-retention-days')
        total_max_conn = command_get_config(self, 'max-dbms-connections')
        region = command_get_config(self, 'region')
        scaling = command_get_config(self, 'scaling')
        support = command_get(self, 'support', 'support stack has been created')
        password = command_get(self, 'postgres_admin_password', 'RDS stack has been created')
        dns_name = command_get_config(self, 'dns-name')
        os.environ['RDS_HOST'] = support['host']
        os.environ['RDS_PORT'] = support['port']
        os.environ['RDS_DBNAME'] = 'veraison'
        os.environ['RDS_USER'] = 'veraison'
        os.environ['RDS_PASSWORD'] = password # pyright: ignore
        os.environ['VTS_PORT'] = str(ports['vts'])
        os.environ['PROVISIONING_PORT'] = str(ports['provisioning'])
        os.environ['VERIFICATION_PORT'] = str(ports['verification'])
        os.environ['MANAGEMENT_PORT'] = str(ports['management'])
        os.environ['VTS_PORT'] = str(ports['vts'])
        os.environ['KEYCLOAK_PORT'] = str(ports['keycloak'])
        os.environ['CW_LOG_RETENTION_DAYS'] = str(retention_days)
        os.environ['MAX_STORE_CONNECTIONS'] = str(total_max_conn // (3 * scaling['max-size'])) # pyright: ignore
        os.environ['DEPLOYMENT_NAME'] = args.deployment_name
        os.environ['AWS_REGION'] = region # pyright: ignore
        os.environ['VERAISON_AWS_DNS_NAME'] = dns_name # pyright: ignore

        servers = command_get_elasticache_servers(self)
        os.environ['ELASTICACHE_SERVERS'] = yaml.dump(servers, default_flow_style=True)

        config_path = command_instantiate_template(
                self, args.deployment_name, args.services_config_template,
                self.cache.dir)
        self.cache['services_config_file'] = config_path

        cw_config_path = command_instantiate_template(
                self, args.deployment_name, args.cloudwatch_config_template)

        command_create_image(self, args, 'services',
                             {
                                 'deb': deb_path,
                                 'config_path': config_path,
                                 'cw_config_path': cw_config_path,
                             })


class ConfigureCommand(BaseCommand):

    name = 'configure'
    desc = 'configure deployment parameters'

    default_provisioning_user = 'veraison-provisioner'
    default_provisioning_password = 'veraison'
    default_management_user = 'veraison-manager'
    default_management_password = 'veraison'
    default_client_id = 'veraison-client'
    default_client_secret = 'YifmabB4cVSPPtFLAmHfq7wKaEHQn10Z'
    default_scaling_min_size = 1
    default_scaling_max_size = 3
    default_scaling_cpu_util_target = 60
    default_scaling_request_count_target = 10
    default_cw_log_retention_days = 30
    default_iam_logger_role_name = 'veraison-logger'
    default_iam_instance_profile_name = 'veraison-service'
    default_iam_permission_boundary_arn = 'arn:aws:iam::aws:policy/AdministratorAccess'
    default_max_dbms_connections = 75

    def update_arguments(self, parser):
        parser.add_argument('-i', '--init', action='store_true',
                            help='indiates that this is an initial configuration for a '
                            'deployment; if a parameter is not specified, its default value '
                            'will be configured, if it has one, or an error will be raised '
                            'otherwise')

        parser.add_argument('-a', '--admin-cidr',
                            help='CIDR for IP address that will be allowed access to the '
                                 'sentinel instance')
        parser.add_argument('-c', '--vpc-cidr',
                            help='CIDR used for the VPC; this should be large ennough to '
                                 'accommodate four subnets (recommended at least /16).')
        parser.add_argument('-d', '--dns-name',
                            help='parent domain that will be used for the deployment; it must '
                                 'be registered in Route53')
        parser.add_argument('-r', '--region',
                            help='AWS region inside which the deployment will be created')

        parser.add_argument('-P', '--provisioning-port' , type=Port,
                            help='port that will be used for the provisioning service')
        parser.add_argument('-V', '--verification-port' , type=Port,
                            help='port that will be used for the verification service')
        parser.add_argument('-M', '--management-port' , type=Port,
                            help='port that will be used for the management service')
        parser.add_argument('-K', '--keycloak-port' , type=Port,
                            help='port that will be used for Keycloak')
        parser.add_argument('-X', '--vts-port' , type=Port,
                            help='port that will be used for the VTS service (note: '
                                 'currently, VTS runs on the same instance as a front-end '
                                 'service so this is largely irrelevant)')

        parser.add_argument('-p', '--provisioning-user',
                            help='the name of provisioning user; must match Keycloak realm')
        parser.add_argument('-R', '--provisioning-password',
                            help='the password of provisioning user; must match Keycloak realm')
        parser.add_argument('-m', '--management-user',
                            help='the name of management user; must match Keycloak realm')
        parser.add_argument('-Q', '--management-password',
                            help='the password of management user; must match Keycloak realm')
        parser.add_argument('-C', '--client-id',
                            help='client ID used to connect to Keycloak API')
        parser.add_argument('-S', '--client-secret',
                            help='client SECRET used to connect to Keycloak API')

        parser.add_argument('-A', '--keycloak-admin',
                            help='the name of Keycloak instance admin account')
        parser.add_argument('-U', '--keycloak-version',
                            help='verision of Keycloak that will be used in the deployment')

        parser.add_argument('-I', '--scaling-min-size', type=int,
                            help='minimum number of instances inside an auto-scaling group')
        parser.add_argument('-J', '--scaling-max-size', type=int,
                            help='maximum number of instances inside an auto-scaling group')
        parser.add_argument('-D', '--scaling-cpu-util-target', type=Percent,
                            help='CPU utilization that will be targed by auto-scaling policies')
        parser.add_argument('-E', '--scaling-request-count-target', type=int,
                            help='request count that will be targed by auto-scaling policies')

        parser.add_argument('-L', '--cw-log-retention-days', type=int,
                            help='number of days logs will be retained in CloudWatch')
        parser.add_argument('-G', '--iam-logger-role-name',
                            help='Name that will be used for logger AIM role')
        parser.add_argument('-F', '--iam-instance-profile-name',
                            help='Name that will be used for service AIM instance profile')
        parser.add_argument('-B', '--iam-permission-boundary-arn',
                            help='ARN of IAM policy to be used as role boundary')
        parser.add_argument('-O', '--max-dbms-connections', type=int,
                            help='Maxium number of parallel DBMS connections that will be created'
                                 'by Veraison; should be divisible by 3')

    def run(self, args):
        self._configure_region(args)
        self._configure_admin_cidr(args)
        self._configure_vpc_cidr(args)
        self._configure_client_settings(args)
        self._configure_dns_name(args)
        self._configure_ports(args)
        self._configure_keycloak(args)
        self._configure_scaling(args)
        self._configure_cloudwatch(args)
        self._configure_dbms(args)

    def _configure_admin_cidr(self, args):
        if args.admin_cidr:
            self.cache['admin-cidr'] = args.admin_cidr
        elif args.init:
            self.logger.warning('setting admin CIDR to 0.0.0.0/0; this is not recommended: '
                                're-run with --admin-cidr option')
            self.cache['admin-cidr'] = '0.0.0.0/0'

    def _configure_vpc_cidr(self, args):
        if args.vpc_cidr:
            self.cache['vpc-cidr'] = args.vpc_cidr
        elif args.init:
            self.logger.info('defaulting VPC CIDR to 10.0.0.0/16 '
                             '(re-run with --vpc-cidr to change)')
            self.cache['admin-cidr'] = '10.0.0.0/16'

    def _configure_client_settings(self, args):
        attrs =  [
            'provisioning_user',
            'provisioning_password',
            'management_user',
            'management_password',
            'client_id',
            'client_secret',
        ]

        if args.init:
            for attr in attrs:
                default = getattr(self.__class__, f'default_{attr}')
                setattr(args, attr, getattr(args, attr) or default)

        client_config = self.cache.get('client_config', {})

        for attr in attrs:
            new_val = getattr(args, attr)
            if new_val:
                client_config[attr] = new_val

        self.cache['client_config'] = client_config

    def _configure_dns_name(self, args):
        if args.dns_name:
            self.logger.debug(f'writing {args.dns_name} to cache')
            self.cache['dns-name'] = args.dns_name
        elif args.init:
            raise ValueError('--dns-name not specified; it must be configured')

    def _configure_region(self, args):
        if args.region:
            self.logger.debug(f'writing {args.region} to cache')
            self.cache['region'] = args.region
            return
        elif not args.init and self.cache.get('region'):
            return  # already set and not re-initializing

        subnet_id = command_get_config(self, 'subnet-id')

        resp = self.aws.ec2.describe_subnets(SubnetIds=[subnet_id])
        zone_id = resp['Subnets'][0]['AvailabilityZoneId']

        resp = self.aws.ec2.describe_availability_zones(ZoneIds=[zone_id])
        region = resp['AvailabilityZones'][0]['RegionName']

        self.cache['region'] = region

    def _configure_ports(self, args):
        names = ['vts', 'keycloak', 'provisioning', 'verification', 'management']
        ports = self.cache.get('ports', {})

        for name in names:
            val = getattr(args, f'{name}_port', None)
            if val is not None:
                ports[name] = int(val)
            elif args.init:
                raise ValueError(f'--{name}-port not specified; it must be configured')

        self.logger.debug('writing ports to cache...')
        self.cache['ports'] = ports

    def _configure_keycloak(self, args):
        if args.keycloak_version:
            self.cache['keycloak-version'] = args.keycloak_version
        elif args.init:
            self.logger.info('setting keycloak version to 25.0.5; '
                             'use --keycloak-version to change')
            self.cache['keycloak-version'] = '25.0.5'

        if args.keycloak_admin:
            self.cache['keycloak-admin'] = args.keycloak_admin
        elif args.init:
            self.logger.info('setting keycloak admin account name to "admin"; '
                             'use --keycloak-admin to change')
            self.cache['keycloak-admin'] = 'admin'

    def _configure_scaling(self, args):
        arg_names = [
            'scaling-min-size',
            'scaling-max-size',
            'scaling-cpu-util-target',
            'scaling-request-count-target',
        ]
        scaling = self.cache.get('scaling', {})

        for name in arg_names:
            attr = name.replace('-', '_')
            val = getattr(args, attr)
            if val:
                scaling[name.replace('scaling-', '')] = val
            elif args.init:
                default = getattr(self, f'default_{attr}', None)
                if default:
                    self.logger.info(f'setting {name.replace('-', ' ')} to {default}; '
                                     f'use --{name} to change')
                    scaling[name.replace('scaling-', '')] = default
                else:
                    self.fail(f'--{name} must be specified')

        self.logger.debug('writing scaling config...')
        self.cache['scaling'] = scaling

    def _configure_cloudwatch(self, args):
        arg_names = [
            'iam-logger-role-name',
            'iam-instance-profile-name',
            'iam-permission-boundary-arn',
        ]
        iam = self.cache.get('iam', {})

        for name in arg_names:
            attr = name.replace('-', '_')
            val = getattr(args, attr)
            if val:
                iam[name.replace('iam-', '')] = val
            elif args.init:
                default = getattr(self, f'default_{attr}', None)
                if default:
                    self.logger.info(f'setting {name.replace('-', ' ')} to {default}; '
                                     f'use --{name} to change')
                    iam[name.replace('iam-', '')] = default
                else:
                    self.fail(f'--{name} must be specified')

        self.logger.debug('writing IAM config...')
        self.cache['iam'] = iam

        if args.cw_log_retention_days:
            self.cache['cw-log-retention-days'] = args.cw_log_retention_days
        elif args.init:
            default = self.default_cw_log_retention_days
            self.logger.info(f'setting CloudWatch log retention to {default}; '
                             'use --cw-log-retention-days to change')
            self.cache['cw-log-retention-days'] = default

    def _configure_dbms(self, args):
        if args.max_dbms_connections:
            self.cache['max-dbms-connections'] = args.max_dbms_connections
        elif args.init:
            self.logger.info('setting max dbms connections to 75; '
                             'use --keycloak-version to change')
            self.cache['max-dbms-connections'] = 75


class CreateKeycloakImageCommand(BaseCommand):

    name = 'create-keycloak-image'
    desc = 'create IMA image for the Keycloak EC2 instance'

    def update_arguments(self, parser):
        parser.add_argument(
                '-c', '--keycloak-conf-template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/keycloak.conf.template',
            )),
        )
        parser.add_argument(
                '-s', '--keycloak-service-template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/keycloak.service.template',
            )),
        )
        parser.add_argument(
            '-t', '--template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/image-keycloak.pkr.hcl',
            )),
        )
        parser.add_argument(
            '-r', '--realm-file',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../misc/veraison-realm.json',
            )),
        )
        parser.add_argument('-T', '--instance-type')

    def run(self, args):
        if not os.path.isfile(args.realm_file):
            self.fail(f'realm file {args.realm_file} does not exist')

        self.logger.debug('generating admin password for Keycloak and writing into cache...')
        # note: not using punctuation in the initial password, as it will be passed through
        # multiple shells, environments, and tools, and we don't want to
        # warry about correctly escaping everything at every stage. Using a longer string
        # to compensate.
        password = randomword(40)
        os.environ['KEYCLOAK_ADMIN_PASSWORD'] = password
        self.cache['keycloak_admin_password'] = password

        support = command_get(self, 'support', 'support stack has been created')
        password = command_get(self, 'postgres_admin_password', 'RDS stack has been created')

        os.environ['RDS_HOST'] = support['host']
        os.environ['RDS_PORT'] = support['port']
        os.environ['RDS_USER'] = 'veraison'
        os.environ['RDS_PASSWORD'] = password  # pyright: ignore

        conf_path = command_instantiate_template(
                self, args.deployment_name, args.keycloak_conf_template)
        service_path = command_instantiate_template(
                self, args.deployment_name, args.keycloak_service_template)

        command_create_image(self, args, 'keycloak',
                             {
                                 'conf_path': conf_path,
                                 'service_path': service_path,
                                 'realm_path': args.realm_file,
                             })


class CreateSentinelImageCommand(BaseCommand):

    name = 'create-sentinel-image'
    desc = 'create IMA image for the sentinel EC2 instance'

    def update_arguments(self, parser):
        parser.add_argument(
            '-t', '--template',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../templates/image-sentinel.pkr.hcl',
            )),
        )
        parser.add_argument(
            '-d', '--dispatcher',
            default=os.path.abspath(os.path.join(
                os.path.dirname(__file__),
                '../misc/sentinel-commands',
            )),
        )
        parser.add_argument('-T', '--instance-type')

    def run(self, args):
        command_create_image(self, args, 'sentinel',
                             {
                                 'command_dispatcher_path': args.dispatcher,
                             })


class DeleteImageCommand(BaseCommand):

    name = 'delete-image'
    desc = 'delete IMA image for the Veraison services EC2 instance'

    def update_arguments(self, parser):
        parser.add_argument('name')

    def run(self, args):
        images = self.cache.get('images', {})
        iid = images.get(args.name)
        if iid is None:
            self.fail(f'no entry for image {args.name} found in the deployment cache')

        self.logger.info(f'deleting image {args.name} ({iid})...')
        self.aws.ec2.deregister_image(ImageId=iid)

        self.logger.debug(f'removing image {args.name} from cache')
        del images[args.name]
        self.cache['images'] = images

        self.logger.info('done.')


class CreateKeyPairCommand(BaseCommand):

    name = 'create-key-pair'
    desc = 'create a key pair that will be used for SSH access to the deployment\'s instances'

    def update_arguments(self, parser):
        parser.add_argument('-n', '--key-name')
        parser.add_argument('-t', '--key-type', choices=['rsa', 'ed25519'], default='rsa')

    def run(self, args):
        key_info = self.cache.get('key')
        if key_info:
            self.fail(f'key pair for {args.deployment_name} already exits: '
                      f'{key_info['name']} ({key_info['id']})')

        name = args.key_name or os.getenv('VERAISON_AWS_KEY') or args.deployment_name

        self.logger.info(f'creating key pair {name} for {args.deployment_name}...')
        resp = aws.ec2.create_key_pair(
            KeyName=name,
            KeyType=args.key_type,
            KeyFormat='pem',
            TagSpecifications=[
                {
                    'ResourceType': 'key-pair',
                    'Tags': [
                        {'Key': 'veraison-deployment', 'Value': args.deployment_name},
                    ],
                }
            ],
        )

        path = os.path.join(self.cache.dir, f'{name}_{args.key_type}')
        self.logger.info(f'writing private key to {path}')
        with open(path, 'w') as wfh:
            wfh.write(resp['KeyMaterial'])
        os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)

        self.cache['key'] ={
            'name': name,
            'id': resp['KeyPairId'],
            'fingerprint': resp['KeyFingerprint'],
            'path': path,
        }

        self.logger.info('done.')


class DeleteKeyPairCommand(BaseCommand):

    name = 'delete-key-pair'
    desc = 'create a key pair that will be used for SSH access to the deployment\'s instances'

    def run(self, args):
        self.logger.info(f'deleting key pair for {args.deployment_name}...')
        key_info = self.cache.get('key')
        if key_info:
            if os.path.isfile(key_info['path']):
                self.logger.debug(f'deleting {key_info['path']}')
                os.remove(key_info['path'])
            else:
                self.logger.debug(f'{key_info['path']} not found (already deleted?)')
            self.logger.debug(f'deleting AWS key pair {key_info['name']} ({key_info['id']})')
            self.aws.ec2.delete_key_pair(KeyPairId=key_info['id'])
            del self.cache['key']
            self.logger.info('done.')
        else:
            self.logger.debug('no key info cached; checking VERAISON_AWS_KEY')
            key_name = os.getenv('VERAISON_AWS_KEY')
            if key_name:
                self.logger.debug(f'deleting AWS key pair {key_name}')
                self.aws.ec2.delete_key_pair(KeyName=key_name)
            else:
                self.logger.debug('VERAISON_AWS_KEY not specified; search for key '
                                  f'tagged with {args.deployment_name}')
                resp = self.aws.ec2.describe_key_pairs(
                    Filters=[{
                        'Name': 'tag:veraison-deployment',
                        'Values': [
                            args.deployment_name,
                        ],
                    }],
                )

                if len(resp['KeyPairs']) == 1:
                    name = resp['KeyPairs'][0]['KeyName']
                    kid = resp['KeyPairs'][0]['KeyPairId']
                    self.logger.debug(f'deleting AWS key pair {name} ({kid})')
                    self.aws.ec2.delete_key_pair(KeyPairId=kid)
                else:
                    if len(resp['KeyPairs']) > 1:
                        names = ', '.join([kp['KeyName'] for kp in resp['KeyPairs']])
                        self.logger.debug(f'multiple key pairs for {args.deployment_name} found '
                                          f'({names}). Specify key name using VERAISON_AWS_KEY')
                    else:
                        self.logger.debug(f'no key pairs found for {args.deployment_name}')

                    self.fail(f'could not delete key pair for {args.deployment_name}')

            self.logger.info('done. (local files not touched)')


class CreateDebCommand(BaseCommand):

    name = 'create-deb'
    desc = 'create the Veraison Debian package'

    def update_arguments(self, parser):
        parser.add_argument(
            '-s', '--veraison-src',
            help='path to Veraison services source; if not specified, '
                 'it will be guess based on this script\'s location',
        )
        parser.add_argument(
            '-w', '--work-dir', default='/tmp',
            help='this will be used as the working directory when creating the .deb. '
                 'Upon completion, t will contain the intermediate artifacts.',
        )

    def run(self, args):
        src_root = args.veraison_src
        if src_root is None:
            src_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))

        script = os.path.join(src_root, 'deployments/debian/deployment.sh')
        if not os.path.isfile(script):
            self.fail(f'script {script} does not exist')

        # Ensure we'll be bulding for amd64 even on Arm hosts, as the EC2
        # instances are x86_64.
        os.environ['GOARCH'] = 'amd64'

        self.logger.info(f'creating Debian package under {args.work_dir}...')
        create_deb_cmd = f'{script} create-deb {args.work_dir}'
        self.logger.debug(create_deb_cmd)
        exit_code, stdout, stderr = run_in_shell(create_deb_cmd, args.verbose)
        if exit_code:
            self.fail_shell('deb creation', exit_code, stdout, stderr)

        regex = re.compile(r"building package 'veraison' in '(?P<deb_path>[^']+)'")
        match = regex.search(stdout)
        if not match:
            self.fail(f'could not find deb path in script output')

        deb_path = match.group('deb_path')
        dest_path = os.path.join(args.cache_dir, os.path.basename(deb_path))
        self.logger.debug(f'moving {deb_path} to {dest_path}')
        shutil.move(deb_path, dest_path)

        self.logger.debug('updating deployment cache')
        self.cache['deb'] = dest_path

        self.logger.info(f'created {dest_path}')

        self.logger.info('done.')


class DeleteDebCommand(BaseCommand):

    name = 'delete-deb'
    desc = 'delete perviously created Debian package'

    def run(self, args):
        self.logger.info('deleting cached Debian package...')

        deb_path = self.cache.get('deb')
        if not deb_path:
            self.logger.info('could not find deb in cache; nothing to do.')
            return

        self.logger.debug(f'removing deb {deb_path}')
        os.remove(deb_path)
        del self.cache['deb']

        self.logger.info('done.')


class AddDebCommand(BaseCommand):

    name = 'add-deb'
    desc = 'Use the specified debian package for the deployment, rather than creating a new one'

    def update_arguments(self, parser):
        parser.add_argument('path', 
                            help='Path to the deb package to add.')
        parser.add_argument('-l', '--link', action='store_true',
                            help='''"Link" the package to the deployment via a cache entry, but
                            do not copy it into the deployment files. I.e. use it in-place.''')

    def run(self, args):
        self.logger.info(f'Adding {args.path} to the deployment...')

        path = args.path
        if not args.link:
            dest_path = os.path.join(args.cache_dir, os.path.basename(path))
            self.logger.debug(f'copying {path} to {dest_path}...')
            shutil.copyfile(path, dest_path)
            path = dest_path

        self.logger.debug('updating deployment cache...')
        self.cache['deb'] = path

        self.logger.info('done.')


class CreateVpcStack(BaseCommand):
    name = 'create-vpc-stack'
    desc = 'create stack setting up the VPC, subnets and associated routing for Veraison services'

    def run(self, args):
        region = self.cache.get('region')
        if not region:
            self.fail('region has not been configured')

        cidr_block = self.cache.get('vpc-cidr')
        if not cidr_block:
            self.fail('VPC CIDR block has not been configured')

        template_path = os.path.abspath(os.path.join(
            os.path.dirname(__file__),
            '../templates/stack-vpc.yaml',
        ))

        extra_params = [
            {'ParameterKey': 'Region', 'ParameterValue': region},
            {'ParameterKey': 'VpcCidrBlock', 'ParameterValue': cidr_block},
        ]

        command_create_stack(cmd, args.deployment_name, 'vpc',
                             template_path, extra_params, args.wait_period)


class ShellCommand(BaseCommand):

    name = 'shell'
    desc = '''start a shell on an EC2 instance associated with the deployment.
    By default, the shell will be started on the stentinel, however -H/--host
    can be used to specify a different instance, in which case the shell
    will be started on that instance, using the sentinel as a jumphost. status
    command can be used to see available instances.
    '''

    def update_arguments(self, parser):
        parser.add_argument('-H', '--host')
        parser.add_argument('-s', '--server-alive-interval', type=int, default=60)

    def run(self, args):
        if not shutil.which('ssh'):
            self.fail('ssh does not appear to be installed on the system')

        host = command_get(self, 'support.sentinel-dns-name', 'RDS stack been created')
        key = command_get(self, 'key.path', 'a key pair been created')

        ssh_opts = '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
        if args.server_alive_interval:
            ssh_opts += f' -o ServerAliveInterval={args.server_alive_interval}'

        if args.host:
            prox_cmd = f'ssh {ssh_opts} -i {key} -W %h:%p ubuntu@{host}'
            ssh_opts += f' -o ProxyCommand="{prox_cmd}"'
            host = args.host

        ssh_cmd = f'ssh {ssh_opts} -i {key} ubuntu@{host}'
        self.logger.debug(ssh_cmd)
        os.system(ssh_cmd)


class DbShellCommand(BaseCommand):

    name = 'dbshell'
    desc = 'start a database shell on a deployment\'s RDS instance'

    def update_arguments(self, parser):
        parser.add_argument('-c', '--db-command',
                            help='command to run in the DB shell (if not specified, an '
                                 'interactive shell will be started).')
        parser.add_argument('-s', '--server-alive-interval', type=int, default=60,
                            help='sets ssh\'s ServerAliveInterval option to periodically '
                                 'keep idle sessions alive')
        parser.add_argument('-t', '--tuples-only', action='store_true',
                            help='when used with -c/--db-command, only output the query '
                                 'result tuples, and omit header/footer')

    def run(self, args):
        if not shutil.which('ssh'):
            self.fail('ssh does not appear to be installed on the system')

        support = command_get(self, 'support', 'support stack has been created')
        pgpass = command_get(self, 'postgres_admin_password', 'RDS stack has been created')
        psql_cmd = f'PGPASSWORD={pgpass} psql -h {support['host']} -p {support['port']} '\
                   f'-U veraison -d veraison'

        if args.tuples_only:
            psql_cmd = f'{psql_cmd} -t'

        if args.db_command:
            psql_cmd = f'{psql_cmd} -c {shlex.quote(args.db_command)}'

        host = command_get(self, 'support.sentinel-dns-name', 'RDS stack been created')
        key = command_get(self, 'key.path', 'a key pair been created')

        ssh_opts = '-t -o StrictHostKeyChecking=no -o LogLevel=ERROR ' +\
                   '-o UserKnownHostsFile=/dev/null'
        if args.server_alive_interval:
            ssh_opts += f' -o ServerAliveInterval={args.server_alive_interval}'

        ssh_cmd = f'ssh {ssh_opts} -i {key} ubuntu@{host} "{psql_cmd}"'
        self.logger.debug(ssh_cmd)
        os.system(ssh_cmd)


class CreateClientConfigCommand(BaseCommand):

    name = 'create-client-config'
    desc = '''
    create configuration for Veraison clients to access the deployment
    '''
    all_clients = ['cocli', 'evcli', 'pocli']

    def update_arguments(self, parser):
        parser.add_argument('-c', '--client',
                            action='append', dest='clients', choices=self.all_clients)
        parser.add_argument('-o', '--output-dir', default=xdg.BaseDirectory.xdg_config_home)

    def run(self, args):
        self.logger.info('creating Veraison client config(s)...')
        client_config = self.cache.get('client_config')
        if not client_config:
            self.fail('client config not found; run configure command with appropriate options')

        dns_name = self.cache.get('dns-name')
        if not dns_name:
            self.fail('DNS name not found; run configure command ensuring VERAISON_AWS_DNS_NAME '
                      'environment variable is set (and exported)')
        host = f'services.{dns_name}'

        services_config_path = self.cache.get('services_config_file')
        if not services_config_path:
            self.fail('services config not found; re-create the services image')

        with open(services_config_path, 'r') as fh:
            srv_cfg = yaml.safe_load(fh)

        kc_host = srv_cfg.get('auth', {}).get('host')
        kc_port = srv_cfg.get('auth', {}).get('port')
        if not (kc_host and kc_port):
            self.fail('keycloak host/port not found in services config; '
                      'has auth been configured?')

        clients = args.clients or self.all_clients
        for client in clients:
            self.logger.info(f'generating {client} config...')
            outdir = os.path.join(args.output_dir, client)
            if not os.path.isdir(outdir):
                self.logger.debug(f'creating {outdir}')
                os.makedirs(outdir)

            config = getattr(self, f'get_{client}_config')(
                srv_cfg, client_config, host, kc_host, kc_port,
            )

            outfile = os.path.join(outdir, 'config.yaml')
            self.logger.debug(f'writing {outfile}')
            with open(outfile, 'w') as wfh:
                yaml.dump(config, wfh)

            self.cache['client_config_dir'] = args.output_dir
            self.logger.info('done.')

    def get_cocli_config(self, srv_cfg, cli_cfg, host, kc_host, kc_port):
        port = int(srv_cfg['provisioning']['listen-addr'].split(':')[1])
        return {
            'api_server': f'https://{host}:{port}/endorsement-provisioning/v1/submit',
            'auth': 'oauth2',
            'username': cli_cfg['provisioning_user'],
            'password': cli_cfg['provisioning_password'],
            'client_id': cli_cfg['client_id'],
            'client_secret': cli_cfg['client_secret'],
            'token_url': f'https://{kc_host}:{kc_port}'
                          '/realms/veraison/protocol/openid-connect/token',
        }

    def get_evcli_config(self, srv_cfg, cli_cfg, host, kc_host, kc_port):
        port = int(srv_cfg['verification']['listen-addr'].split(':')[1])
        return {
            'api_server': f'https://{host}:{port}/challenge-response/v1/newSession',
        }

    def get_pocli_config(self, srv_cfg, cli_cfg, host, kc_host, kc_port):
        port = int(srv_cfg['management']['listen-addr'].split(':')[1])
        return {
            'tls': True,
            'host': host,
            'port': port,
            'auth': 'oauth2',
            'username': cli_cfg['management_user'],
            'password': cli_cfg['management_password'],
            'client_id': cli_cfg['client_id'],
            'client_secret': cli_cfg['client_secret'],
            'token_url': f'https://{kc_host}:{kc_port}'
                          '/realms/veraison/protocol/openid-connect/token',
        }


class CacheCommand(BaseCommand):

    name = 'cache'
    desc = 'show cached info for the deployment'

    def update_arguments(self, parser):
        parser.add_argument('-q', '--query',
                            help='a dot-separated path for the entry the value of which is '
                                 'to be displayed; if not specified, the entrie cache will be '
                                 'shown')
        parser.add_argument('-d', '--delete',
                            help='the key to be deleted (note: dot-spearated paths '
                                 'not supported)')

    def run(self, args):
        if args.query:
            val = self.cache.get(args.query)
            if val is not None:
                sys.stdout.write(val)
        elif args.delete:
            del self.cache[args.delete]
        else:
            print(f'deployment: {args.deployment_name}')
            pprint.pp(self.cache.as_dict())


class SetupRdsCommand(BaseCommand):

    name = 'setup-rds'
    desc = 'setup the RDS instance for use as a K-V store for the services'

    def run(self, args):
        support = command_get(self, 'support', 'support stack has been created')
        password = command_get(self, 'postgres_admin_password', 'RDS stack has been created')
        res = command_sentinel(
            self,
            f'setup-rds {support['host']} {support['port']} {password}',
            verbose=args.verbose, echo=False, hide=False, pty=True,
        )
        if res.exited != 0:
            self.fail(f'could not set up stores; got {res.exited}: {res.stdout}')


class CheckStoresCommand(BaseCommand):

    name = 'check-stores'
    desc = 'output the contents of deployment\'s sqlite3 stores'
    aliases = ['stores']

    def run(self, args):
        support = command_get(self, 'support', 'support stack has been created')
        password = command_get(self, 'postgres_admin_password', 'RDS stack has been created')
        res = command_sentinel(
            self,
            f'check-stores {support['host']} {support['port']} {password}',
            verbose=args.verbose, echo=False, hide=False, pty=True,
        )
        if res.exited != 0:
            self.fail(f'could not check stores; got {res.exited}: {res.stdout}')


class ClearStoresCommand(BaseCommand):
    name = 'clear-stores'
    desc = 'clear the contents of deployment\'s sqlite3 stores'

    def run(self, args):
        support = command_get(self, 'support', 'support stack has been created')
        password = command_get(self, 'postgres_admin_password', 'RDS stack has been created')
        res = command_sentinel(
            self,
            f'clear-stores {support['host']} {support['port']} {password}',
            verbose=args.verbose, echo=False, hide=False, pty=True,
        )
        if res.exited != 0:
            self.fail(f'could not clear stores; got {res.exited}: {res.stdout}')


class StatusCommand(BaseCommand):

    name = 'status'
    desc = 'show status of the deployment'

    def run(self, args):
        print(f'deployment: {args.deployment_name}')
        print()

        print('images:')
        images = self.cache.get('images', {})
        if 'keycloak' in images:
            print(f'  keycloak: {images['keycloak']}')
        else:
            print(f'  keycloak: {COLOR_GREY}not created{COLOR_RESET}')
        if 'services' in images:
            print(f'  services: {images['services']}')
        else:
            print(f'  services: {COLOR_GREY}not created{COLOR_RESET}')
        if 'sentinel' in images:
            print(f'  sentinel: {images['sentinel']}')
        else:
            print(f'  sentinel: {COLOR_GREY}not created{COLOR_RESET}')
        print()


        print('stacks:')
        if 'vpc' in self.cache:
            print(f'       vpc stack: {COLOR_GREEN}created{COLOR_RESET}')
        else:
            print(f'       vpc stack: {COLOR_GREY}not created{COLOR_RESET}')

        if 'support' in self.cache:
            print(f'   support stack: {COLOR_GREEN}created{COLOR_RESET}')
        else:
            print(f'   support stack: {COLOR_GREY}not created{COLOR_RESET}')

        if 'services' in self.cache:
            print(f'  services stack: {COLOR_GREEN}created{COLOR_RESET}')
        else:
            print(f'  services stack: {COLOR_GREY}not created{COLOR_RESET}')
        print()

        if 'support' in self.cache:
            support = self.cache['support']
            print(f'RDS instance:\n  {support['host']}:{support['port']}')
            print()
            servers = command_get_elasticache_servers(self)
            print(f'ElastiCache servers:')
            for server in servers:
                print(f'  {server}')
            print()


        if 'services' in self.cache:
            instances = command_get_ec2_instances(self)
            if instances:
                print('EC2 instances:')
                for instance_type, instances in instances.items():
                    for instance in instances:
                        host = instance['PrivateDnsName']
                        id = instance['InstanceId']
                        print(f'  {host} {id}\t({instance_type})')
                print()

                services = command_get(self, 'services', 'services stack has been crated')
                host = services['services-dns-name']
                port = services['verification-port']

                resp = requests.get(f'https://{host}:{port}/.well-known/veraison/verification')
                well_known = json.loads(resp.text)

                print(f'Veraison version: {well_known['version']}')
                print()

                print('EAR verification key:')
                print(json.dumps(well_known['ear-verification-key'], indent=4))
                print()

                print('Supported verification media types:')
                for mt in well_known['media-types']:
                    print(f'\t{mt}')
                print()

                client_config = command_get(self, 'client_config',
                                            'create-client-config been run')
                client_id = client_config['client_id']
                client_secret = client_config['client_secret']
                username = client_config['provisioning_user']
                password = client_config['provisioning_password']

                kc_host = services['keycloak-dns-name']
                kc_port = services['keycloak-port']

                oauth = OAuth2Session(client=LegacyApplicationClient(client_id=client_id))
                oauth.fetch_token(
                    token_url=f'https://{kc_host}:{kc_port}' + \
                               '/realms/veraison/protocol/openid-connect/token',
                    username=username, password=password,
                    client_id=client_id, client_secret=client_secret,
                )

                port = services['provisioning-port']
                resp = oauth.get(f'https://{host}:{port}/.well-known/veraison/provisioning')
                well_known = json.loads(resp.text)

                print('Supported provisioning media types:')
                for mt in well_known['media-types']:
                    print(f'\t{mt}')
                print()


class RestartCwAgentCommand(BaseCommand):

    name = 'restart-cwagent'
    desc = 'restart CloudWatch agent on running services instances'

    def update_arguments(self, parser):
        parser.add_argument('-D', '--delete-logs', action='store_true',
                            help='delete instance\'s logs before restarting the agent')

    def run(self, args):
        self.logger.info('restarting CloudWatch agent on service EC2 instances...')

        command_restart_cw_agents(self, delete_logs=args.delete_logs)

        self.logger.info('done.')


class FollowCommand(BaseCommand):

    name = 'follow'
    desc = 'live stream latest logged event for the specified service'

    def update_arguments(self, parser):
        parser.add_argument('service',
                            choices=['vts', 'provisioning', 'verification', 'management'],
                            help='service whos logs will be followed')
        parser.add_argument('-s', '--start', default='10 mins ago',
                            help='show history from this point before following live events')

    def run(self, args):
        self.logger.info(f'following logs for {args.service}...')
        group_name = f'{args.deployment_name}-{args.service}'

        self.logger.debug(f'getting history for {group_name}...')
        events = command_get_cw_logs(self, args.deployment_name, args.service, args.start)
        for event in events:
            print_cw_event(event)

        self.logger.debug(f'identifying ARN of log group {group_name}...')
        resp = self.aws.logs.describe_log_groups(logGroupNamePrefix=args.deployment_name)
        arn = None
        for group in resp['logGroups']:
            if group['logGroupName'] == group_name:
                arn = group['arn']
                if arn.endswith(':*'):
                    arn = arn[:-2]
                break
        else:
            self.fail(f'Could not find log group for {args.service}')

        self.logger.debug(f'streaming events for {arn}...')
        resp = self.aws.logs.start_live_tail(logGroupIdentifiers=[arn])
        try:
            stream = iter(resp['responseStream'])
            next(stream) # skip sessionStart event
            for event in stream:
                results = event.get('sessionUpdate', {}).get('sessionResults', [])
                for res in results:
                    print_cw_event(res)
        except KeyboardInterrupt:
            pass
        finally:
            resp['responseStream'].close()


        self.logger.info('done.')


class GetLogsCommand(BaseCommand):

    name = 'get-logs'
    aliases = ['logs']
    desc = 'Download services logs into the specified directory'

    all_services=['vts', 'provisioning', 'verification', 'management']

    def update_arguments(self, parser):
        parser.add_argument('outdir',
                            help='output directory into which logs will be donloaded')
        parser.add_argument('-s', '--start', help='get logs from this point')
        parser.add_argument('-e', '--end', help='get logs up to this point')
        parser.add_argument('-S' '--service', action='append', dest='services',
                            choices=self.all_services,
                            help='only get logs for this services (may be specified '
                                 'multiple times)')

    def run(self, args):
        services = args.services or self.all_services
        svc_str = ', '.join(services)
        self.logger.info(f'getting logs for {svc_str}...')

        if os.path.isfile(args.outdir):
            self.fail('{args.outdir} is a file')
        elif not os.path.isdir(args.outdir):
            self.logger.debug(f'creating {args.outdir}...')
            os.makedirs(args.outdir)
        else:
            self.logger.debug(f'writing into existing {args.outdir}...')

        for service in services:
            outfile = os.path.join(args.outdir, f'{service}-stdout.log')
            group_name = f'{args.deployment_name}-{service}'

            self.logger.info(f'getting events for {group_name}...')
            events = command_get_cw_logs(self, args.deployment_name, service,
                                         args.start, args.end)

            with open(outfile, 'w') as wfh:
                for event in events:
                    ts = datetime.fromtimestamp(event['ingestionTime']/1000, UTC)
                    inst = event['logStreamName']
                    message = event['message']
                    wfh.write(f'{ts} {inst} {message}\n')

            self.logger.debug(f'written {outfile}')

        self.logger.info('done.')


class DeleteLogsCommand(BaseCommand):

    name = 'delete-logs'
    aliases = ['clear-logs']
    desc = 'Delete services logs in CloudWatch'

    all_services=['vts', 'provisioning', 'verification', 'management']

    def update_arguments(self, parser):
        parser.add_argument('-f', '--force-agent-restart', action='store_true',
                            help='force restart of CloudWatch agents even if no log groups '
                                 'were deleted')
        parser.add_argument('-S', '--service', action='append', dest='services',
                            choices=self.all_services,
                            help='only get logs for this services (may be specified '
                                 'multiple times)')

    def run(self, args):
        services = args.services or self.all_services
        svc_str = ', '.join(services)
        self.logger.info(f'deleting logs for {svc_str}...')

        need_agent_restart = args.force_agent_restart
        for service in services:
            log_group = f'{args.deployment_name}-{service}'
            self.logger.debug(f'deleting log group {log_group}...')
            try:
                self.aws.logs.delete_log_group(logGroupName=log_group)
                need_agent_restart = True
            except self.aws.logs.exceptions.ResourceNotFoundException:
                pass

        if need_agent_restart:
            self.logger.info('restarting CloudWatch agents...')
            command_restart_cw_agents(self, delete_logs=True)

        self.logger.info('done.')


class CreateSignerKeyCommand(BaseCommand):

    name = 'create-signer-key'
    desc = 'Create EAR signer key and add it to AWS Secrets Manager'

    ec_curves=['P-256', 'P-384', 'P-521', 'secp256k1']
    okp_curves=['Ed25519', 'Ed448', 'X25519', 'X448']

    def update_arguments(self, parser):
        parser.add_argument('-t', '--kty', default='EC',
                            choices=['EC', 'RSA', 'OKP', 'oct'],
                            help='Key type')
        parser.add_argument('-c', '--crv', default='P-256',
                            choices=(self.ec_curves + self.okp_curves),
                            help='''Curve to be used when generating the key. This is only used
                                 if kty is EC or OK. Valid curves for EC are P-256, P-384, P-521,
                                 and secp256k1. Valid curves for OKP are Ed25519, Ed448, X25519,
                                 and X448,''')
        parser.add_argument('-s', '--size', type=int, default=4096,
                            help='''Size, in bits, of the generated key. This is inly used if
                                 kty is RSA or oct''')

    def run(self, args):
        self.logger.info('Creating signer key...')

        name = f'{args.deployment_name}-ear-signer'
        params = dict(
            kid=name,
            kty=args.kty,
            use='sig',
            key_ops=['sign'],
        )

        if args.kty == 'EC':
            if args.crv not in self.ec_curves:
                self.fail(f'curve "{args.crv}" for kty EC')
            params['crv'] = args.crv
        if args.kty == 'OKP':
            if args.crv not in self.okp_curves:
                self.fail(f'curve "{args.crv}" for kty OKP')
            params['crv'] = args.crv
        else:
            if args.size < 2048:
                self.fail('key size of at least 2048 bits must be used')
            params['size'] = args.size

        self.logger.debug(f'params: {params}')

        key = jwk.JWK.generate(**params)

        resp = self.aws.secretsmanager.create_secret(
            Name=name,
            SecretString=key.export_private(),
            Tags=[
                {'Key': 'veraison-deployment', 'Value': args.deployment_name},
            ],
        )
        self.cache['signer-key-arn'] = resp['ARN']

        self.logger.info('Done.')


class DeleteSignerKeyCommand(BaseCommand):

    name = 'delete-signer-key'
    desc = 'Delete previously created EAR signer key from AWS Secrets Manager'

    def run(self, args):
        self.logger.info('Creating signer key...')

        self.aws.secretsmanager.delete_secret(
            SecretId=f'{args.deployment_name}-ear-signer',
            ForceDeleteWithoutRecovery=True,
        )

        del self.cache['signer-key-arn']

        self.logger.info('Done.')


class Port(int):
    def __init__(self, val):
        val = int(val)
        if val < 1 or val > 65535:
            raise ValueError(f'must in range [1-65535]')
        self = val


class Percent(int):
    def __init__(self, val):
        val = int(val)
        if val < 0 or val > 100:
            raise ValueError(f'must in range [0-100]')
        self = val


class StoreIntList(argparse.Action):

    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, [int(v) for v in values.split(',')])  # pyright: ignore


class LogFormatter(logging.Formatter):

    fmt = f'{{}}%(asctime)s %(name)s %(levelname)s{COLOR_RESET}: %(message)s'

    level_formats = {
        logging.DEBUG: fmt.format(COLOR_DARK_GREY),
        logging.INFO: fmt.format(COLOR_MEDIUM_GREY),
        logging.WARNING: fmt.format(COLOR_YELLOW),
        logging.ERROR: fmt.format(COLOR_RED),
        logging.CRITICAL: fmt.format(COLOR_BOLD_RED),
    }

    def format(self, record):
        log_fmt = self.level_formats.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)


if __name__ == '__main__':
    handler = logging.StreamHandler()
    handler.setLevel(logging.DEBUG)
    handler.setFormatter(LogFormatter())
    logging.basicConfig(level=logging.INFO, handlers=[handler])
    logging.getLogger('botocore').setLevel(logging.WARNING)
    logging.getLogger("paramiko").setLevel(logging.WARNING)

    aws = Aws(
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY'),
        aws_secret_access_key=os.getenv('AWS_SECRET_KEY'),
        aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
        profile_name=os.getenv('AWS_PROFILE'),
    )

    cmd_map = {}
    for name, cmd_cls in inspect.getmembers(
            sys.modules[__name__],
            lambda x: inspect.isclass(x) and issubclass(x, BaseCommand) and x is not BaseCommand):
        if not name[0].isupper():
            continue  # ignore variable bindings
        assert cmd_cls.name, f'{cmd_cls} does not define a name'
        cmd = cmd_cls(aws)
        assert cmd.name not in cmd_map, f'duplicate name {cmd.name}'
        cmd_map[cmd.name] = cmd
        for alias in cmd.aliases:
            assert alias not in cmd_map, f'duplicate alias {alias}'
            cmd_map[alias] = cmd

    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--deployment-name',
                        help='the name for this deployment; this is used in a number '
                             'places, including AWS resources tags')
    parser.add_argument('-f', '--force', action='store_true',
                        help='force overwrite of exiting resources')
    parser.add_argument('-W', '--wait-period', type=int, default=1,
                        help='period (in seconds) to wait between polls to AWS for '
                             'long-running command progress')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='show DEBUG level messages')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='hide INFO level messages')
    parser.add_argument(
        '--cache-dir', default=xdg.BaseDirectory.save_data_path('veraison/aws'),
        help='location that will be used for local deployment data',
    )
    parser.add_argument(
        '--no-error', action='store_false', dest='fail_with_error',
        help='do not report command failures as errors',
    )

    subparsers = parser.add_subparsers(dest='command', required=True)
    for name, command in cmd_map.items():
        if name == command.name:
            command.register(subparsers)

    args = parser.parse_args()

    os.makedirs(args.cache_dir, exist_ok=True)
    current_deployment_path = os.path.join(args.cache_dir, 'current_deployment')
    if args.deployment_name:
        with open(current_deployment_path, 'w') as wfh:
            wfh.write(args.deployment_name)
    else:
        if os.path.isfile(current_deployment_path):
            with open(current_deployment_path) as fh:
                args.deployment_name = fh.read().strip()
        else:
            logging.critical('no current deployment exists; '
                             'please use -d/--deployment-name to specify')
            sys.exit(1)

    cmd = cmd_map[args.command]
    cmd.fail_with_error = args.fail_with_error
    try:
        cmd.execute(args)
    except Exception as e:
        write = cmd.logger.critical if args.fail_with_error else cmd.logger.info
        write(f'{e.__class__.__name__}: {e}')
